# -*- coding: utf-8 -*-
"""Predicting Movie Success..!.ipynb project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kvSEu21dEX-1B0W4xUD3YclZWh_ypwjs

# Categorising the IMDB rating into 3 classes Hit,Avg,Flop
Here I have dataset named movie_metadata in which the target variable is IMDB score and other variables that decide the IMDB score. Instead of just IMDB score,With the help of other parameters I want to predict whether a movie is Hit,Avg or Flop.

imdb_score	Classify
1-3	    Flop Movie
3-6	    Average Movie
6-10	Hit Movie
"""

#Importing necessary Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""#  Describing Data
The dataset contains 28 variables for 5043 movies, spanning across
100 years in 66 countries. There are 2399 unique director names, and
thousands of actors/actresses. “imdb_score” is
the response variable while the other 27 variables are possible predictors.
"""

path= r"C:\Users\suraj tripathi\Desktop\movie_metadata.csv"

df=pd.read_csv(path)

df

df.describe()

df.info()

object_columns = df.select_dtypes(include="object").columns
object_columns

#Categorising the target varible
bins = [ 1, 3, 6, 10]
labels = ['FLOP', 'AVG', 'HIT']
df['imdb_binned'] = pd.cut(df['imdb_score'], bins=bins, labels=labels)

df['imdb_binned']

#Barplot of imbd_binned column
df.groupby(['imdb_binned']).size().plot(kind="bar",fontsize=14)
plt.xlabel('Categories')
plt.ylabel('Number of Movies')
plt.title('Categorization of Movies')

#Checking the new column
df.head(5)

#Shape of the dataset
df.shape

"""# Handling the Missing values
Every datset have some missing values, lets find out in which cloumns they are?
"""

#Total null values present in each column
df.isnull().sum()

#Droping the samples that have missing values
df.dropna(inplace=True)

df.shape

#Describing the categorical data
df.describe(include='object')

#Dropping 2 columns
df.drop(columns=['movie_title','movie_imdb_link'],inplace=True)

"""# Label Encoding
All the categorical columns and the columns with text data are
being Label Encodeded in this step.
"""

#Label encoding the categorical columns
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
cat_list=['color', 'director_name', 'actor_2_name',
        'genres', 'actor_1_name',
        'actor_3_name',
        'plot_keywords',
        'language', 'country', 'content_rating',
       'title_year', 'aspect_ratio']
df[cat_list]=df[cat_list].apply(lambda x:le.fit_transform(x))

df[cat_list]

#Finding Correlation between variables
corr = df.corr()

plt.subplots(figsize=(20,15))
sns.heatmap(corr,  cmap='RdYlGn',annot=True)

"""# These variables that are correlated cause errors in the
prediction, so removing them
"""

#Removing few columns due to multicollinearity
df.drop(columns=['cast_total_facebook_likes','num_critic_for_reviews'],inplace=True)

"""
Removing the column "imdb_score" since we have "imdb_binned

I am gonna train the model with imdb_binned not with imdb_score so dropping the column."""

#Removing the column "imdb_score" since we have "imdb_binned"
df.drop(columns=['imdb_score'],inplace=True)

"""# CLASSIFICATION MODEL BUILDING

Splitting the data into X and y where X contains Indepentent variables and y contain Target/Dependent variable.

"""

#Independent Variables
X = df.iloc[:, 0:23].values
#Dependent/Target Variable
y = df.iloc[:, 23].values

"""# Train Test Split
We need data not only to train our model but also to test our model. So splitting the dataset into 70:30 (Train:Test) ratio.We have a predefined a function
in Sklearn library called test_train_split, lets use that.
"""

#Spliting the data into train and test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0,stratify = y)
print(X_train.shape)
print(y_train.shape)

"""# Scaling
Few variables will be in the range of Millions and some in Tens,
lets bring all of them into same scale
"""

#Scaling the dependent variables
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""#  Random Forest
Random forests is an ensemble learning method for classification that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) of the individual trees

n_estimators is a parameter that specify number of trees in the forest.

criterion is to specify what function to measure the quality of a split. “entropy” is for the information gain.
"""

#Training the Random Forest Classifer on Train data
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)

#Predicting the target variable
y_pred = classifier.predict(X_test)

y_pred

"""# Confusion Matrix
Confusion matrix gives a clear view of ground truth and prediction.
"""

#Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test,y_pred)
cm

"""# Classification Report"""

#Classification Report
from sklearn.metrics import classification_report
cr = classification_report(y_test,y_pred)
print(cr)

